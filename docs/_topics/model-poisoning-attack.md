---
title: Model Poisoning Attack
layout: topic
permalink: /model-poisoning-attack/
---

# Model Poisoning Attack

<details markdown="1">
<summary><strong>Table of Contents</strong></summary>

<details markdown="1">
<summary>NeurIPS</summary>

  - [2024](#neurips-2024)
  - [2023](#neurips-2023)
  - [2021](#neurips-2021)

</details>

<details markdown="1">
<summary>ICML</summary>

  - [2025](#icml-2025)
  - [2024](#icml-2024)
  - [2023](#icml-2023)
  - [2021](#icml-2021)

</details>

<details markdown="1">
<summary>ICLR</summary>

  - [2025](#iclr-2025)
  - [2020](#iclr-2020)

</details>

<details markdown="1">
<summary>NDSS</summary>

  - [2021](#ndss-2021)

</details>

<details markdown="1">
<summary>CVPR</summary>

  - [2025](#cvpr-2025)
  - [2024](#cvpr-2024)

</details>

<details markdown="1">
<summary>IJCAI</summary>

  - [2024](#ijcai-2024)
  - [2023](#ijcai-2023)
  - [2022](#ijcai-2022)

</details>

<details markdown="1">
<summary>AAAI</summary>

  - [2023](#aaai-2023)
  - [2016](#aaai-2016)

</details>

<details markdown="1">
<summary>AISTATS</summary>

  - [2022](#aistats-2022)

</details>

<details markdown="1">
<summary>KDD</summary>

  - [2024](#kdd-2024)
  - [2022](#kdd-2022)
  - [2021](#kdd-2021)
  - [2015](#kdd-2015)

</details>

<details markdown="1">
<summary>CCS</summary>

  - [2025](#ccs-2025)
  - [2022](#ccs-2022)

</details>

<details markdown="1">
<summary>ACL</summary>

  - [2024](#acl-2024)
  - [2020](#acl-2020)

</details>

<details markdown="1">
<summary>NAACL-HLT</summary>

  - [2021](#naacl-hlt-2021)

</details>

<details markdown="1">
<summary>EMNLP</summary>

  - [2021](#emnlp-2021)

</details>

<details markdown="1">
<summary>SIGIR</summary>

  - [2024](#sigir-2024)

</details>

<details markdown="1">
<summary>ICDE</summary>

  - [2022](#icde-2022)

</details>

<details markdown="1">
<summary>WWW</summary>

  - [2025](#www-2025)
  - [2023](#www-2023)

</details>

<details markdown="1">
<summary>SP</summary>

  - [2025](#sp-2025)
  - [2024](#sp-2024)
  - [2023](#sp-2023)

</details>

<details markdown="1">
<summary>USENIX Security Symposium</summary>

  - [2024](#usenix-security-symposium-2024)
  - [2020](#usenix-security-symposium-2020)

</details>

<details markdown="1">
<summary>ACM Multimedia</summary>

  - [2023](#acm-multimedia-2023)

</details>

<details markdown="1">
<summary>IEEE Trans. Neural Networks Learn. Syst.</summary>

  - [2025](#ieee-trans-neural-networks-learn-syst-2025)

</details>

<details markdown="1">
<summary>IEEE Trans. Artif. Intell.</summary>

  - [2024](#ieee-trans-artif-intell-2024)

</details>

<details markdown="1">
<summary>IEEE Trans. Knowl. Data Eng.</summary>

  - [2025](#ieee-trans-knowl-data-eng-2025)
  - [2024](#ieee-trans-knowl-data-eng-2024)

</details>

<details markdown="1">
<summary>IEEE Trans. Emerg. Top. Comput.</summary>

  - [2024](#ieee-trans-emerg-top-comput-2024)

</details>

<details markdown="1">
<summary>IEEE Trans. Inf. Forensics Secur.</summary>

  - [2025](#ieee-trans-inf-forensics-secur-2025)
  - [2024](#ieee-trans-inf-forensics-secur-2024)
  - [2023](#ieee-trans-inf-forensics-secur-2023)
  - [2022](#ieee-trans-inf-forensics-secur-2022)
  - [2021](#ieee-trans-inf-forensics-secur-2021)

</details>

<details markdown="1">
<summary>IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.</summary>

  - [2022](#ieee-trans-comput-aided-des-integr-circuits-syst-2022)

</details>

<details markdown="1">
<summary>IEEE Trans. Computers</summary>

  - [2025](#ieee-trans-computers-2025)
  - [2023](#ieee-trans-computers-2023)

</details>

<details markdown="1">
<summary>Nat. Mac. Intell.</summary>

  - [2024](#nat-mac-intell-2024)

</details>

<details markdown="1">
<summary>Inf. Sci.</summary>

  - [2025](#inf-sci-2025)
  - [2023](#inf-sci-2023)

</details>

<details markdown="1">
<summary>Expert Syst. Appl.</summary>

  - [2023](#expert-syst-appl-2023)

</details>

<details markdown="1">
<summary>Neural Networks</summary>

  - [2024](#neural-networks-2024)

</details>

</details>


---

## NeurIPS <a id="venue-neurips"></a>

<details>

<summary>Expand NeurIPS</summary>

<a id="neurips-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models.</td>
<td>NeurIPS</td>
<td>2024</td>
<td><a href="http://papers.nips.cc/paper_files/paper/2024/hash/95dcc1f6463491d37a8918c1d38380a7-Abstract-Conference.html">Link</a></td>
</tr>
<tr>
<td>Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models.</td>
<td>NeurIPS</td>
<td>2024</td>
<td><a href="http://papers.nips.cc/paper_files/paper/2024/hash/97d008f7873b8dd55cb6dd343fc4386f-Abstract-Conference.html">Link</a></td>
</tr>
<tr>
<td>Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models.</td>
<td>NeurIPS</td>
<td>2024</td>
<td><a href="http://papers.nips.cc/paper_files/paper/2024/hash/6a2e30664b9647f97d7b9275358d083c-Abstract-Conference.html">Link</a></td>
</tr>
</tbody></table>

<a id="neurips-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>RECESS Vaccine for Federated Learning: Proactive Defense Against Model Poisoning Attacks.</td>
<td>NeurIPS</td>
<td>2023</td>
<td><a href="http://papers.nips.cc/paper_files/paper/2023/hash/1b80fe066fdbceb3a2960117bac33917-Abstract-Conference.html">Link</a></td>
</tr>
</tbody></table>

<a id="neurips-2021"></a>
<h3 class="year-heading">2021</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective.</td>
<td>NeurIPS</td>
<td>2021</td>
<td><a href="https://proceedings.neurips.cc/paper/2021/hash/692baebec3bb4b53d7ebc3b9fabac31b-Abstract.html">Link</a></td>
</tr>
</tbody></table>

</details>

## ICML <a id="venue-icml"></a>

<details>

<summary>Expand ICML</summary>

<a id="icml-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>PoisonedEye: Knowledge Poisoning Attack on Retrieval-Augmented Generation based Large Vision-Language Models.</td>
<td>ICML</td>
<td>2025</td>
<td><a href="https://openreview.net/forum?id=6SIymOqJlc">Link</a></td>
</tr>
</tbody></table>

<a id="icml-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>FedREDefense: Defending against Model Poisoning Attacks for Federated Learning using Model Update Reconstruction Error.</td>
<td>ICML</td>
<td>2024</td>
<td><a href="https://openreview.net/forum?id=Wjq2bS7fTK">Link</a></td>
</tr>
<tr>
<td>The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline.</td>
<td>ICML</td>
<td>2024</td>
<td><a href="https://openreview.net/forum?id=ZvFLbEPv6x">Link</a></td>
</tr>
</tbody></table>

<a id="icml-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Exploring Model Dynamics for Accumulative Poisoning Discovery.</td>
<td>ICML</td>
<td>2023</td>
<td><a href="https://proceedings.mlr.press/v202/zhu23d.html">Link</a></td>
</tr>
<tr>
<td>Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks.</td>
<td>ICML</td>
<td>2023</td>
<td><a href="https://proceedings.mlr.press/v202/lu23e.html">Link</a></td>
</tr>
<tr>
<td>LeadFL: Client Self-Defense against Model Poisoning in Federated Learning.</td>
<td>ICML</td>
<td>2023</td>
<td><a href="https://proceedings.mlr.press/v202/zhu23j.html">Link</a></td>
</tr>
<tr>
<td>Poisoning Language Models During Instruction Tuning.</td>
<td>ICML</td>
<td>2023</td>
<td><a href="https://proceedings.mlr.press/v202/wan23b.html">Link</a></td>
</tr>
</tbody></table>

<a id="icml-2021"></a>
<h3 class="year-heading">2021</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Model-Targeted Poisoning Attacks with Provable Convergence.</td>
<td>ICML</td>
<td>2021</td>
<td><a href="http://proceedings.mlr.press/v139/suya21a.html">Link</a></td>
</tr>
</tbody></table>

</details>

## ICLR <a id="venue-iclr"></a>

<details>

<summary>Expand ICLR</summary>

<a id="iclr-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing.</td>
<td>ICLR</td>
<td>2025</td>
<td><a href="https://openreview.net/forum?id=RzUvkI3p1D">Link</a></td>
</tr>
</tbody></table>

<a id="iclr-2020"></a>
<h3 class="year-heading">2020</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks.</td>
<td>ICLR</td>
<td>2020</td>
<td><a href="https://openreview.net/forum?id=SyevYxHtDB">Link</a></td>
</tr>
</tbody></table>

</details>

## NDSS <a id="venue-ndss"></a>

<details>

<summary>Expand NDSS</summary>

<a id="ndss-2021"></a>
<h3 class="year-heading">2021</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning.</td>
<td>NDSS</td>
<td>2021</td>
<td><a href="https://www.ndss-symposium.org/ndss-paper/manipulating-the-byzantine-optimizing-model-poisoning-attacks-and-defenses-for-federated-learning/">Link</a></td>
</tr>
</tbody></table>

</details>

## CVPR <a id="venue-cvpr"></a>

<details>

<summary>Expand CVPR</summary>

<a id="cvpr-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Model Poisoning Attacks to Federated Learning via Multi-Round Consistency.</td>
<td>CVPR</td>
<td>2025</td>
<td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Model_Poisoning_Attacks_to_Federated_Learning_via_Multi-Round_Consistency_CVPR_2025_paper.html">Link</a></td>
</tr>
<tr>
<td>Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models.</td>
<td>CVPR</td>
<td>2025</td>
<td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Silent_Branding_Attack_Trigger-free_Data_Poisoning_Attack_on_Text-to-Image_Diffusion_CVPR_2025_paper.html">Link</a></td>
</tr>
</tbody></table>

<a id="cvpr-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Semantic Shield: Defending Vision-Language Models Against Backdooring and Poisoning via Fine-Grained Knowledge Alignment.</td>
<td>CVPR</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/CVPR52733.2024.02344">Link</a></td>
</tr>
</tbody></table>

</details>

## IJCAI <a id="venue-ijcai"></a>

<details>

<summary>Expand IJCAI</summary>

<a id="ijcai-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>EAB-FL: Exacerbating Algorithmic Bias through Model Poisoning Attacks in Federated Learning.</td>
<td>IJCAI</td>
<td>2024</td>
<td><a href="https://www.ijcai.org/proceedings/2024/51">Link</a></td>
</tr>
</tbody></table>

<a id="ijcai-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning.</td>
<td>IJCAI</td>
<td>2023</td>
<td><a href="https://doi.org/10.24963/ijcai.2023/508">Link</a></td>
</tr>
</tbody></table>

<a id="ijcai-2022"></a>
<h3 class="year-heading">2022</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Poisoning Deep Learning Based Recommender Model in Federated Learning Scenarios.</td>
<td>IJCAI</td>
<td>2022</td>
<td><a href="https://doi.org/10.24963/ijcai.2022/306">Link</a></td>
</tr>
</tbody></table>

</details>

## AAAI <a id="venue-aaai"></a>

<details>

<summary>Expand AAAI</summary>

<a id="aaai-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>DeFL: Defending against Model Poisoning Attacks in Federated Learning via Critical Learning Periods Awareness.</td>
<td>AAAI</td>
<td>2023</td>
<td><a href="https://doi.org/10.1609/aaai.v37i9.26271">Link</a></td>
</tr>
</tbody></table>

<a id="aaai-2016"></a>
<h3 class="year-heading">2016</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Data Poisoning Attacks against Autoregressive Models.</td>
<td>AAAI</td>
<td>2016</td>
<td><a href="https://doi.org/10.1609/aaai.v30i1.10237">Link</a></td>
</tr>
</tbody></table>

</details>

## AISTATS <a id="venue-aistats"></a>

<details>

<summary>Expand AISTATS</summary>

<a id="aistats-2022"></a>
<h3 class="year-heading">2022</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification.</td>
<td>AISTATS</td>
<td>2022</td>
<td><a href="https://proceedings.mlr.press/v151/panda22a.html">Link</a></td>
</tr>
</tbody></table>

</details>

## KDD <a id="venue-kdd"></a>

<details>

<summary>Expand KDD</summary>

<a id="kdd-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>FedRoLA: Robust Federated Learning Against Model Poisoning via Layer-based Aggregation.</td>
<td>KDD</td>
<td>2024</td>
<td><a href="https://doi.org/10.1145/3637528.3671906">Link</a></td>
</tr>
</tbody></table>

<a id="kdd-2022"></a>
<h3 class="year-heading">2022</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients.</td>
<td>KDD</td>
<td>2022</td>
<td><a href="https://doi.org/10.1145/3534678.3539231">Link</a></td>
</tr>
</tbody></table>

<a id="kdd-2021"></a>
<h3 class="year-heading">2021</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Data Poisoning Attacks Against Outcome Interpretations of Predictive Models.</td>
<td>KDD</td>
<td>2021</td>
<td><a href="https://doi.org/10.1145/3447548.3467405">Link</a></td>
</tr>
</tbody></table>

<a id="kdd-2015"></a>
<h3 class="year-heading">2015</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Predictive Modeling for Public Health: Preventing Childhood Lead Poisoning.</td>
<td>KDD</td>
<td>2015</td>
<td><a href="https://doi.org/10.1145/2783258.2788629">Link</a></td>
</tr>
</tbody></table>

</details>

## CCS <a id="venue-ccs"></a>

<details>

<summary>Expand CCS</summary>

<a id="ccs-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling.</td>
<td>CCS</td>
<td>2025</td>
<td><a href="https://doi.org/10.1145/3719027.3744845">Link</a></td>
</tr>
</tbody></table>

<a id="ccs-2022"></a>
<h3 class="year-heading">2022</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets.</td>
<td>CCS</td>
<td>2022</td>
<td><a href="https://doi.org/10.1145/3548606.3560554">Link</a></td>
</tr>
</tbody></table>

</details>

## ACL <a id="venue-acl"></a>

<details>

<summary>Expand ACL</summary>

<a id="acl-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models.</td>
<td>ACL</td>
<td>2024</td>
<td><a href="https://doi.org/10.18653/v1/2024.acl-long.140">Link</a></td>
</tr>
</tbody></table>

<a id="acl-2020"></a>
<h3 class="year-heading">2020</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Weight Poisoning Attacks on Pretrained Models.</td>
<td>ACL</td>
<td>2020</td>
<td><a href="https://doi.org/10.18653/v1/2020.acl-main.249">Link</a></td>
</tr>
</tbody></table>

</details>

## NAACL-HLT <a id="venue-naacl-hlt"></a>

<details>

<summary>Expand NAACL-HLT</summary>

<a id="naacl-hlt-2021"></a>
<h3 class="year-heading">2021</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Concealed Data Poisoning Attacks on NLP Models.</td>
<td>NAACL-HLT</td>
<td>2021</td>
<td><a href="https://doi.org/10.18653/v1/2021.naacl-main.13">Link</a></td>
</tr>
</tbody></table>

</details>

## EMNLP <a id="venue-emnlp"></a>

<details>

<summary>Expand EMNLP</summary>

<a id="emnlp-2021"></a>
<h3 class="year-heading">2021</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning.</td>
<td>EMNLP</td>
<td>2021</td>
<td><a href="https://doi.org/10.18653/v1/2021.emnlp-main.241">Link</a></td>
</tr>
</tbody></table>

</details>

## SIGIR <a id="venue-sigir"></a>

<details>

<summary>Expand SIGIR</summary>

<a id="sigir-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Revisit Targeted Model Poisoning on Federated Recommendation: Optimize via Multi-objective Transport.</td>
<td>SIGIR</td>
<td>2024</td>
<td><a href="https://doi.org/10.1145/3626772.3657764">Link</a></td>
</tr>
</tbody></table>

</details>

## ICDE <a id="venue-icde"></a>

<details>

<summary>Expand ICDE</summary>

<a id="icde-2022"></a>
<h3 class="year-heading">2022</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>FedRecAttack: Model Poisoning Attack to Federated Recommendation.</td>
<td>ICDE</td>
<td>2022</td>
<td><a href="https://doi.org/10.1109/ICDE53745.2022.00243">Link</a></td>
</tr>
</tbody></table>

</details>

## WWW <a id="venue-www"></a>

<details>

<summary>Expand WWW</summary>

<a id="www-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Model Supply Chain Poisoning: Backdooring Pre-trained Models via Embedding Indistinguishability.</td>
<td>WWW</td>
<td>2025</td>
<td><a href="https://doi.org/10.1145/3696410.3714624">Link</a></td>
</tr>
</tbody></table>

<a id="www-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>MaSS: Model-agnostic, Semantic and Stealthy Data Poisoning Attack on Knowledge Graph Embedding.</td>
<td>WWW</td>
<td>2023</td>
<td><a href="https://doi.org/10.1145/3543507.3583203">Link</a></td>
</tr>
</tbody></table>

</details>

## SP <a id="venue-sp"></a>

<details>

<summary>Expand SP</summary>

<a id="sp-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Preference Poisoning Attacks on Reward Model Learning.</td>
<td>SP</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/SP61157.2025.00094">Link</a></td>
</tr>
</tbody></table>

<a id="sp-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models.</td>
<td>SP</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/SP54263.2024.00207">Link</a></td>
</tr>
<tr>
<td>Test-Time Poisoning Attacks Against Test-Time Adaptation Models.</td>
<td>SP</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/SP54263.2024.00072">Link</a></td>
</tr>
<tr>
<td>TrojanPuzzle: Covertly Poisoning Code-Suggestion Models.</td>
<td>SP</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/SP54263.2024.00140">Link</a></td>
</tr>
</tbody></table>

<a id="sp-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Benchmarking the Effect of Poisoning Defenses on the Security and Bias of Deep Learning Models.</td>
<td>SP</td>
<td>2023</td>
<td><a href="https://doi.org/10.1109/SPW59333.2023.00010">Link</a></td>
</tr>
</tbody></table>

</details>

## USENIX Security Symposium <a id="venue-usenix security symposium"></a>

<details>

<summary>Expand USENIX Security Symposium</summary>

<a id="usenix security symposium-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning.</td>
<td>USENIX Security Symposium</td>
<td>2024</td>
<td><a href="https://www.usenix.org/conference/usenixsecurity24/presentation/xu-zhangchen">Link</a></td>
</tr>
</tbody></table>

<a id="usenix security symposium-2020"></a>
<h3 class="year-heading">2020</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Local Model Poisoning Attacks to Byzantine-Robust Federated Learning.</td>
<td>USENIX Security Symposium</td>
<td>2020</td>
<td><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/fang">Link</a></td>
</tr>
</tbody></table>

</details>

## ACM Multimedia <a id="venue-acm multimedia"></a>

<details>

<summary>Expand ACM Multimedia</summary>

<a id="acm multimedia-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning.</td>
<td>ACM Multimedia</td>
<td>2023</td>
<td><a href="https://doi.org/10.1145/3581783.3612108">Link</a></td>
</tr>
</tbody></table>

</details>

## IEEE Trans. Neural Networks Learn. Syst. <a id="venue-ieee trans. neural networks learn. syst."></a>

<details>

<summary>Expand IEEE Trans. Neural Networks Learn. Syst.</summary>

<a id="ieee trans. neural networks learn. syst.-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Defending Against Neural Network Model Inversion Attacks via Data Poisoning.</td>
<td>IEEE Trans. Neural Networks Learn. Syst.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TNNLS.2025.3554656">Link</a></td>
</tr>
<tr>
<td>Leverage Variational Graph Representation for Model Poisoning on Federated Learning.</td>
<td>IEEE Trans. Neural Networks Learn. Syst.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TNNLS.2024.3394252">Link</a></td>
</tr>
<tr>
<td>Secure and Efficient Federated Learning Against Model Poisoning Attacks in Horizontal and Vertical Data Partitioning.</td>
<td>IEEE Trans. Neural Networks Learn. Syst.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TNNLS.2024.3486028">Link</a></td>
</tr>
</tbody></table>

</details>

## IEEE Trans. Artif. Intell. <a id="venue-ieee trans. artif. intell."></a>

<details>

<summary>Expand IEEE Trans. Artif. Intell.</summary>

<a id="ieee trans. artif. intell.-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Attacking-Distance-Aware Attack: Semi-targeted Model Poisoning on Federated Learning.</td>
<td>IEEE Trans. Artif. Intell.</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/TAI.2023.3280155">Link</a></td>
</tr>
</tbody></table>

</details>

## IEEE Trans. Knowl. Data Eng. <a id="venue-ieee trans. knowl. data eng."></a>

<details>

<summary>Expand IEEE Trans. Knowl. Data Eng.</summary>

<a id="ieee trans. knowl. data eng.-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>HidAttack: An Effective and Undetectable Model Poisoning Attack to Federated Recommenders.</td>
<td>IEEE Trans. Knowl. Data Eng.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TKDE.2024.3522763">Link</a></td>
</tr>
</tbody></table>

<a id="ieee trans. knowl. data eng.-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>BIC-Based Mixture Model Defense Against Data Poisoning Attacks on Classifiers: A Comprehensive Study.</td>
<td>IEEE Trans. Knowl. Data Eng.</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/tkde.2024.3365548">Link</a></td>
</tr>
</tbody></table>

</details>

## IEEE Trans. Emerg. Top. Comput. <a id="venue-ieee trans. emerg. top. comput."></a>

<details>

<summary>Expand IEEE Trans. Emerg. Top. Comput.</summary>

<a id="ieee trans. emerg. top. comput.-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Blockchain-Based Federated Learning With SMPC Model Verification Against Poisoning Attack for Healthcare Systems.</td>
<td>IEEE Trans. Emerg. Top. Comput.</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/TETC.2023.3268186">Link</a></td>
</tr>
</tbody></table>

</details>

## IEEE Trans. Inf. Forensics Secur. <a id="venue-ieee trans. inf. forensics secur."></a>

<details>

<summary>Expand IEEE Trans. Inf. Forensics Secur.</summary>

<a id="ieee trans. inf. forensics secur.-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>DamPa: Dynamic Adaptive Model Poisoning Attack in Federated Learning.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TIFS.2025.3631449">Link</a></td>
</tr>
<tr>
<td>Enhanced Model Poisoning Attack and Multi-Strategy Defense in Federated Learning.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TIFS.2025.3555193">Link</a></td>
</tr>
<tr>
<td>FLGuardian: Defending Against Model Poisoning Attacks via Fine-Grained Detection in Federated Learning.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TIFS.2025.3570119">Link</a></td>
</tr>
<tr>
<td>FedGhost: Data-Free Model Poisoning Enhancement in Federated Learning.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TIFS.2025.3539087">Link</a></td>
</tr>
<tr>
<td>Maximizing Uncertainty for Federated Learning via Bayesian Optimization-Based Model Poisoning.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TIFS.2025.3531143">Link</a></td>
</tr>
<tr>
<td>PoisonPatch: Natural Adversarial Patches via Diffusion Models and Federated Learning Poisoning.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TIFS.2025.3620155">Link</a></td>
</tr>
<tr>
<td>Securing Federated Learning Against Extreme Model Poisoning Attacks via Multidimensional Time Series Anomaly Detection on Local Updates.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TIFS.2025.3608671">Link</a></td>
</tr>
<tr>
<td>The Gradient Puppeteer: Adversarial Domination in Gradient Leakage Attacks Through Model Poisoning.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TIFS.2025.3607271">Link</a></td>
</tr>
</tbody></table>

<a id="ieee trans. inf. forensics secur.-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>A Robust Privacy-Preserving Federated Learning Model Against Model Poisoning Attacks.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/TIFS.2024.3420126">Link</a></td>
</tr>
<tr>
<td>Data-Agnostic Model Poisoning Against Federated Learning: A Graph Autoencoder Approach.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/TIFS.2024.3362147">Link</a></td>
</tr>
<tr>
<td>MODEL: A Model Poisoning Defense Framework for Federated Learning via Truth Discovery.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/TIFS.2024.3461449">Link</a></td>
</tr>
<tr>
<td>Secure Model Aggregation Against Poisoning Attacks for Cross-Silo Federated Learning With Robustness and Fairness.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2024</td>
<td><a href="https://doi.org/10.1109/TIFS.2024.3416042">Link</a></td>
</tr>
</tbody></table>

<a id="ieee trans. inf. forensics secur.-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>A Manifold Consistency Interpolation Method of Poisoning Attacks Against Semi-Supervised Model.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2023</td>
<td><a href="https://doi.org/10.1109/TIFS.2023.3268882">Link</a></td>
</tr>
<tr>
<td>Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2023</td>
<td><a href="https://doi.org/10.1109/TIFS.2023.3244107">Link</a></td>
</tr>
</tbody></table>

<a id="ieee trans. inf. forensics secur.-2022"></a>
<h3 class="year-heading">2022</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>ShieldFL: Mitigating Model Poisoning Attacks in Privacy-Preserving Federated Learning.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2022</td>
<td><a href="https://doi.org/10.1109/TIFS.2022.3169918">Link</a></td>
</tr>
</tbody></table>

<a id="ieee trans. inf. forensics secur.-2021"></a>
<h3 class="year-heading">2021</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>With Great Dispersion Comes Greater Resilience: Efficient Poisoning Attacks and Defenses for Linear Regression Models.</td>
<td>IEEE Trans. Inf. Forensics Secur.</td>
<td>2021</td>
<td><a href="https://doi.org/10.1109/TIFS.2021.3087332">Link</a></td>
</tr>
</tbody></table>

</details>

## IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. <a id="venue-ieee trans. comput. aided des. integr. circuits syst."></a>

<details>

<summary>Expand IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.</summary>

<a id="ieee trans. comput. aided des. integr. circuits syst.-2022"></a>
<h3 class="year-heading">2022</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Enhancing Reliability and Security: A Configurable Poisoning PUF Against Modeling Attacks.</td>
<td>IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.</td>
<td>2022</td>
<td><a href="https://doi.org/10.1109/TCAD.2022.3197529">Link</a></td>
</tr>
</tbody></table>

</details>

## IEEE Trans. Computers <a id="venue-ieee trans. computers"></a>

<details>

<summary>Expand IEEE Trans. Computers</summary>

<a id="ieee trans. computers-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>BaDFL: Mitigating Model Poisoning in Decentralized Federated Learning.</td>
<td>IEEE Trans. Computers</td>
<td>2025</td>
<td><a href="https://doi.org/10.1109/TC.2025.3603683">Link</a></td>
</tr>
</tbody></table>

<a id="ieee trans. computers-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Model Poisoning Attack on Neural Network Without Reference Data.</td>
<td>IEEE Trans. Computers</td>
<td>2023</td>
<td><a href="https://doi.org/10.1109/TC.2023.3280133">Link</a></td>
</tr>
</tbody></table>

</details>

## Nat. Mac. Intell. <a id="venue-nat. mac. intell."></a>

<details>

<summary>Expand Nat. Mac. Intell.</summary>

<a id="nat. mac. intell.-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Poisoning medical knowledge using large language models.</td>
<td>Nat. Mac. Intell.</td>
<td>2024</td>
<td><a href="https://doi.org/10.1038/s42256-024-00899-3">Link</a></td>
</tr>
</tbody></table>

</details>

## Inf. Sci. <a id="venue-inf. sci."></a>

<details>

<summary>Expand Inf. Sci.</summary>

<a id="inf. sci.-2025"></a>
<h3 class="year-heading">2025</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>TPFL: Privacy-preserving personalized federated learning mitigates model poisoning attacks.</td>
<td>Inf. Sci.</td>
<td>2025</td>
<td><a href="https://doi.org/10.1016/j.ins.2025.121901">Link</a></td>
</tr>
</tbody></table>

<a id="inf. sci.-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Compromise privacy in large-batch Federated Learning via model poisoning.</td>
<td>Inf. Sci.</td>
<td>2023</td>
<td><a href="https://doi.org/10.1016/j.ins.2023.119421">Link</a></td>
</tr>
<tr>
<td>Model poisoning attack in differential privacy-based federated learning.</td>
<td>Inf. Sci.</td>
<td>2023</td>
<td><a href="https://doi.org/10.1016/j.ins.2023.02.025">Link</a></td>
</tr>
</tbody></table>

</details>

## Expert Syst. Appl. <a id="venue-expert syst. appl."></a>

<details>

<summary>Expand Expert Syst. Appl.</summary>

<a id="expert syst. appl.-2023"></a>
<h3 class="year-heading">2023</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Bandit-based data poisoning attack against federated learning for autonomous driving models.</td>
<td>Expert Syst. Appl.</td>
<td>2023</td>
<td><a href="https://doi.org/10.1016/j.eswa.2023.120295">Link</a></td>
</tr>
</tbody></table>

</details>

## Neural Networks <a id="venue-neural networks"></a>

<details>

<summary>Expand Neural Networks</summary>

<a id="neural networks-2024"></a>
<h3 class="year-heading">2024</h3>
<table class="paper-table">
<colgroup>
<col style="width: 60%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 10%">
</colgroup>
<thead><tr>
<th>Title</th>
<th> Venue</th>
<th> Year </th>
<th>Link</th>
</tr></thead>
<tbody>
<tr>
<td>Learning a robust foundation model against clean-label data poisoning attacks at downstream tasks.</td>
<td>Neural Networks</td>
<td>2024</td>
<td><a href="https://doi.org/10.1016/j.neunet.2023.10.034">Link</a></td>
</tr>
</tbody></table>

</details>
